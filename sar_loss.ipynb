{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic6QhTCSQr4Y"
      },
      "source": [
        "# Testing a SAR-based ship classifier with different loss functions\n",
        "\n",
        "In this notebook we explored six different Loss functions on SAR Data, we choose 3 classes of already available data and test the hypothesis of loss functions. The hypothesis is, Loss functions should be selected on the basis of dataset and task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZtt09MPRClz"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MpNDfWkQpzn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZvYq84kRK0K"
      },
      "source": [
        "Define data transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ga_KJaCRLKy"
      },
      "outputs": [],
      "source": [
        "# Define data transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-nLG1IKRTez"
      },
      "source": [
        "Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2vKN-aORVbM"
      },
      "outputs": [],
      "source": [
        "def load_data(address):\n",
        "  # Load Fusar dataset\n",
        "  dataset = ImageFolder(root=address, transform=transform_train)\n",
        "\n",
        "  # Create a dictionary of class names\n",
        "  class_names = {i: classname for i, classname in enumerate(dataset.classes)}\n",
        "\n",
        "  # Split dataset into train and test sets\n",
        "  train_size = int(0.8 * len(dataset))\n",
        "  test_size = len(dataset) - train_size\n",
        "  train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,\n",
        "                            num_workers=2,  # Experiment with different values as recommended above\n",
        "                            # pin_memory=False, # if torch.cuda.is_available() else False,\n",
        "                            persistent_workers=True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False,\n",
        "                            num_workers=2,  # Experiment with different values as recommended above\n",
        "                            # pin_memory=False, # if torch.cuda.is_available() else False,\n",
        "                            persistent_workers=True)\n",
        "  print(\"Top classes indices:\", class_names)\n",
        "  len(train_loader)*64, len(test_loader)*64, len(train_loader)*64+ len(test_loader)*64, dataset\n",
        "\n",
        "  return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0xvbdrvRYsr"
      },
      "source": [
        "Model Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKUIuxsMRamh"
      },
      "outputs": [],
      "source": [
        "# Define CNN model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 28 * 28, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, 3)  # 3 output classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TgR26beetEt",
        "outputId": "5057098a-05ad-4eaa-ee19-1acd684399b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsYM8pV_uj5O",
        "outputId": "366bd3d8-d719-4ba0-add7-de3922ed7f54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "CNNModel                                 [64, 3]                   --\n",
              "├─Sequential: 1-1                        [64, 256, 28, 28]         --\n",
              "│    └─Conv2d: 2-1                       [64, 64, 224, 224]        1,792\n",
              "│    └─ReLU: 2-2                         [64, 64, 224, 224]        --\n",
              "│    └─MaxPool2d: 2-3                    [64, 64, 112, 112]        --\n",
              "│    └─Conv2d: 2-4                       [64, 128, 112, 112]       73,856\n",
              "│    └─ReLU: 2-5                         [64, 128, 112, 112]       --\n",
              "│    └─MaxPool2d: 2-6                    [64, 128, 56, 56]         --\n",
              "│    └─Conv2d: 2-7                       [64, 256, 56, 56]         295,168\n",
              "│    └─ReLU: 2-8                         [64, 256, 56, 56]         --\n",
              "│    └─MaxPool2d: 2-9                    [64, 256, 28, 28]         --\n",
              "├─Sequential: 1-2                        [64, 3]                   --\n",
              "│    └─Linear: 2-10                      [64, 1024]                205,521,920\n",
              "│    └─ReLU: 2-11                        [64, 1024]                --\n",
              "│    └─Dropout: 2-12                     [64, 1024]                --\n",
              "│    └─Linear: 2-13                      [64, 3]                   3,075\n",
              "==========================================================================================\n",
              "Total params: 205,895,811\n",
              "Trainable params: 205,895,811\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 137.44\n",
              "==========================================================================================\n",
              "Input size (MB): 38.54\n",
              "Forward/backward pass size (MB): 2877.82\n",
              "Params size (MB): 823.58\n",
              "Estimated Total Size (MB): 3739.94\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "summary(model, input_size=(64, 3, 224, 224))  # Assuming 3-channel images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvV64u8MRdjo"
      },
      "source": [
        "# FUSAR Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV9v9D-_jDQg"
      },
      "source": [
        "Evaluation on different loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwBvjIK6ERj-"
      },
      "outputs": [],
      "source": [
        "# importing the zipfile module\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zip_file = \"/content/fusar.zip\"\n",
        "path = \"/content/fusar_data/\"\n",
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(zip_file, 'r') as zObject:\n",
        "\n",
        "    # Extracting all the members of the zip\n",
        "    # into a specific location.\n",
        "    zObject.extractall(\n",
        "        path=path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2THTAKQjFPV"
      },
      "outputs": [],
      "source": [
        "# Baseline loss function\n",
        "\n",
        "loss_functions = {\n",
        "    'CrossEntropyLoss': nn.CrossEntropyLoss(),\n",
        "}\n",
        "train_loader, test_loader = load_data('fusar_data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9-Kg0EejGbI"
      },
      "source": [
        "Code for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AijSUfCTjHeg"
      },
      "outputs": [],
      "source": [
        "# Train the model with different loss functions\n",
        "for loss_name, loss_func in loss_functions.items():\n",
        "    print(f\"Training with {loss_name} loss:\")\n",
        "    model = CNNModel()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    model.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):  # Train for 10 epochs\n",
        "        model.train()\n",
        "        print(epoch)\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "            data, target = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(data)\n",
        "\n",
        "            loss = loss_func(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 57 == 0:\n",
        "                print(f'Epoch {epoch + 1}/{10}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}')\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy with {loss_name} loss function: {accuracy:.2f}%\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6MuqKk-ul8I"
      },
      "source": [
        "Comparing different functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJWfisoduevn"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        pt = torch.softmax(input, dim=1)\n",
        "        pt = torch.argmax(pt, dim=1).float()\n",
        "        ce_loss = nn.functional.cross_entropy(input, target, reduction='none')\n",
        "        pt_with_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "        return pt_with_loss.mean()\n",
        "\n",
        "loss_functions = {\n",
        "    \"BCEWithLogitsLoss\":nn.BCEWithLogitsLoss(),\n",
        "    'MSELoss': nn.MSELoss(),\n",
        "    'L1Loss': nn.L1Loss(),\n",
        "    'Focal_loss': FocalLoss(),\n",
        "    'KLDiv': nn.KLDivLoss(reduction=\"batchmean\", log_target=True),\n",
        "    # Add more loss functions here if needed\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nA6v0zUCRnQ_"
      },
      "outputs": [],
      "source": [
        "# Train the model with different loss functions\n",
        "for loss_name, loss_func in loss_functions.items():\n",
        "    print(f\"Training with {loss_name} loss:\")\n",
        "    model = CNNModel()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    model.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):  # Train for 10 epochs\n",
        "        model.train()\n",
        "        print(epoch)\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "            data, target = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(data)\n",
        "\n",
        "            target = nn.functional.one_hot(target, num_classes=3).float()\n",
        "\n",
        "            loss = loss_func(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 57 == 0:\n",
        "                print(f'Epoch {epoch + 1}/{10}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}')\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy with {loss_name} loss function: {accuracy:.2f}%\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIAr4Nx0qltK"
      },
      "source": [
        "# Evaluating OpenSarShip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqO_3zBhKT_9"
      },
      "outputs": [],
      "source": [
        "# importing the zipfile module\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zip_file = \"/content/opensarship_png.zip\"\n",
        "path = \"/content/opensarship_png/\"\n",
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(zip_file, 'r') as zObject:\n",
        "\n",
        "    # Extracting all the members of the zip\n",
        "    # into a specific location.\n",
        "    zObject.extractall(\n",
        "        path=path)\n",
        "\n",
        "\n",
        "train_loader, test_loader = load_data('/content/opensarship_png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MHR2miSoEks",
        "outputId": "eb8afa47-44e8-4d80-9285-5f1c04b953dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top classes indices: {0: 'Cargo', 1: 'Fishing', 2: 'Tanker'}\n"
          ]
        }
      ],
      "source": [
        "# Baseline loss function\n",
        "\n",
        "loss_functions = {\n",
        "    'CrossEntropyLoss': nn.CrossEntropyLoss(),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRF5FPViqicO"
      },
      "outputs": [],
      "source": [
        "# Train the model with different loss functions\n",
        "for loss_name, loss_func in loss_functions.items():\n",
        "    print(f\"Training with {loss_name} loss:\")\n",
        "    model = CNNModel()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    model.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):  # Train for 10 epochs\n",
        "        model.train()\n",
        "        print(epoch)\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "            data, target = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(data)\n",
        "\n",
        "            loss = loss_func(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 57 == 0:\n",
        "                print(f'Epoch {epoch + 1}/{10}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}')\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy with {loss_name} loss function: {accuracy:.2f}%\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_GfBz-4DXdk"
      },
      "outputs": [],
      "source": [
        "# Train the model with different loss functions\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        pt = torch.softmax(input, dim=1)\n",
        "        pt = torch.argmax(pt, dim=1).float()\n",
        "        ce_loss = nn.functional.cross_entropy(input, target, reduction='none')\n",
        "        pt_with_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "        return pt_with_loss.mean()\n",
        "\n",
        "loss_functions = {\n",
        "    \"BCEWithLogitsLoss\":nn.BCEWithLogitsLoss(),\n",
        "    'MSELoss': nn.MSELoss(),\n",
        "    'L1Loss': nn.L1Loss(),\n",
        "    'Focal_loss': FocalLoss(),\n",
        "    'KLDiv': nn.KLDivLoss(reduction=\"batchmean\", log_target=True),\n",
        "    # Add more loss functions here if needed\n",
        "}\n",
        "\n",
        "for loss_name, loss_func in loss_functions.items():\n",
        "    print(f\"Training with {loss_name} loss:\")\n",
        "    model = CNNModel()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    model.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):  # Train for 10 epochs\n",
        "        model.train()\n",
        "        print(epoch)\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "            data, target = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(data)\n",
        "\n",
        "            target = nn.functional.one_hot(target, num_classes=3).float()\n",
        "\n",
        "            loss = loss_func(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 57 == 0:\n",
        "                print(f'Epoch {epoch + 1}/{10}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}')\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy with {loss_name} loss function: {accuracy:.2f}%\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
